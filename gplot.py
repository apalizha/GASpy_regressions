'''
This module contains functions to create plots from the data generated by GASpy
and the regressions created by GASpy_regressions.

Note that many of the plots require Excel files of input data. The formatting
of the Excel files is important. You should follow the templates in the repo.
'''

__author__ = 'Kevin Tran'
__email__ = 'ktran@andrew.cmu.edu'

import pdb  # noqa: F401
import sys
import numpy as np
import pandas as pd
from plotly.offline import init_notebook_mode, iplot
import plotly.graph_objs as go
import regressor as gaspy_regressor     # noqa: F401
sys.path.insert(0, '../')
from gaspy import utils, defaults   # noqa: E402


def volcano(regressor, adsorbate, sheetname, excel_file_path='volcanos_parsed.xlsx',
            regressor_block='no_block', fp_blocks='default', scale='linear', title=None,
            xlabel='adsorption energy (eV)', ylabel='activity', jupyter=True,
            include_results=False, descriptor='energy', vasp_settings=None,
            energy_min=-4, energy_max=4, f_max=0.5,
            ads_move_max=1.5, bare_slab_move_max=0.5, slab_move_max=1.5):
    '''
    Create a volcano plot for the CO2 reduction reaction. This function assumes that
    the Excel worksheet for the CO2 volcano is named 'CO2RR'.

    Inputs:
        regressor           An instance of a fitted GASpyRegressor class
        adsorbate           A string indicating the adsorbate that you want to be studying.
                            If you want more than one adsorbate, then you should probably
                            make another function.
        sheetname           A string indicating the name of the Excel
                            worksheet that contains the volcano information
        excel_file_path     A string indicating the location of the properly-formatted
                            excel file that contains the incumbent volcano
        regressor_block     If your regression model is blocked, you'll need to specify
                            which block you want to use to make the predictions. This
                            will probably be a tuple of strings, like ('CO',).
        fp_blocks           A list of fingerprints over which to find minima
                            (reference the `_minimize_over` function). If you don't want to
                            block at all, then you may set it to `None`
        scale               A string indicating the scale of the volcano you want to read/make.
                            'linear' or 'logarithmic' are good guesses.
        title               A string for the plot title
        xlabel              A string for the x-axis label
        ylabel              A string for the y-axis label
        jupyter             A boolean indicating whether or not you are making this plot
                            in a Jupyter notebook.
        include_results     A boolean indicating whether or not you only want to include
                            simulation results along with the regressor's predictions.
                            Note that if you include simulation results, then cataloged
                            points that correspond to the results database will be
                            filtered out with the `gaspy.unsimulated_catalog` function.
        descriptor          A string indicating the descriptor you are using to
                            create the volcano. This only matters if `include_results`
                            is `True`, because this argument is used to pull out
                            the appropriate information from the results database.
        All of the other inputs are filter settings for the data that we want
        to pull from the database of results. These only matter when
        `include_results == True`.
    Outputs:
        cat_pdocs   The parsed mongo documents of the filtered catalog
        cat_x       The regressor's predicitons for whatever descriptor we chose
        ads_pdocs   The parsed mongo documents of the filtered results database
                    Only returns if `include_results == True`
        ads_x       The simulated values for whatever descriptor we chose.
                    Only returns if `include_results == True`
    '''
    # pylint: disable=too-many-arguments, no-member, too-many-statements, too-many-branches
    # Some of our defaults need to be lists, which are mutable. So we define them
    # down here.
    if fp_blocks == 'default':
        fp_blocks = ['mpid', 'miller', 'top']

    # Load the literature volcano data
    volcano, x_expt, y_expt, label_expt = \
        _pull_literature_volcano(excel_file_path, 'CO2RR', scale=scale)

    # If we're including the results in the volcano, then pull all the documents.
    if include_results:
        # Define the fingerprints to pull from the database, and add the descriptor
        # to make sure that we get it out

        # Note that we filter the "bad" documents from the results database
        # by specifying a lot of filters.
        with utils.get_adsorption_db() as ads_client:
            fingerprints = defaults.fingerprints(simulated=True)    # pylint: disable=E1123
            ads_docs, ads_pdocs = utils.get_docs(ads_client, 'adsorption',
                                                 adsorbates=[adsorbate],
                                                 fingerprints=fingerprints,
                                                 vasp_settings=vasp_settings,
                                                 energy_min=energy_min,
                                                 energy_max=energy_max,
                                                 f_max=f_max,
                                                 ads_move_max=ads_move_max,
                                                 bare_slab_move_max=bare_slab_move_max,
                                                 slab_move_max=slab_move_max)
            ads_x = np.array(ads_pdocs[descriptor])
        cat_docs, cat_pdocs = utils.unsimulated_catalog(adsorbates=[adsorbate],
                                                        vasp_settings=vasp_settings,
                                                        fingerprints=defaults.fingerprints())
    # If we're including only the catalog, then pull only the catalog information
    else:
        with utils.get_catalog_db() as cat_client:
            cat_docs, cat_pdocs = utils.get_docs(cat_client, 'catalog',
                                                 fingerprints=defaults.fingerprints())

    # Catalog documents don't have any information about adsorbates. But if our model
    # requires information about adsorbates, then we probably need to put it in.
    # Note that we have an EAFP wrapper to check whether our model is hierarchical or not.
    try:
        features = regressor.features + regressor.features_inner
    except AttributeError:
        features = regressor.features
    if 'ads' in features:
        cat_pdocs['adsorbates'] = [[adsorbate]] * len(cat_docs)
    # Create the regressor's prediction
    cat_x = regressor.predict(cat_pdocs, regressor_block)

    # Filter the data over each fingerprint block, as per the `_minimize_over` function.
    if fp_blocks:
        if include_results:
            # If we are including the results, then we need to pool the catalog and results
            # together before finding the minima. This ensures that each block will only
            # have one data point show up on the plot. But before we pool them together,
            # we add a new "fingerprint" to their p_docs so that we can keep track of which
            # data come from where after they've been pooled.
            for i, doc in enumerate(cat_docs):
                doc['catalog?'] = True
                cat_docs[i] = doc
            for i, doc in enumerate(ads_docs):
                doc['catalog?'] = False
                ads_docs[i] = doc
            pooled_x = np.concatenate((cat_x, ads_x), axis=0)
            pooled_docs = cat_docs + ads_docs
            # After pooling, we call `_minimizer_over`
            pooled_docs, pooled_x = _minimize_over(fp_blocks, pooled_docs, pooled_x)
            # Now un-pool so that we can differentiate the data in the final plot
            cat_x = []
            ads_x = []      # pylint: disable=redefined-variable-type
            cat_docs = []
            ads_docs = []
            for i, x in enumerate(pooled_x):
                if pooled_docs[i]['catalog?']:
                    cat_x.append(x)
                    cat_docs.append(pooled_docs[i])
                else:
                    ads_x.append(x)
                    ads_docs.append(pooled_docs[i])
        else:
            cat_docs, cat_x = _minimize_over(fp_blocks, cat_docs, cat_x)

    # Plot the experimental data points, the regressor's predictions, and then
    # (if necessary) the results from the database
    traces = [go.Scatter(x=x_expt, y=y_expt, name='Experimental points',
                         mode='markers', text=label_expt)]
    traces.append(_make_trace(cat_x, volcano, cat_docs, label='Regressor predictions'))
    if include_results:
        traces.append(_make_trace(ads_x, volcano, ads_docs, label='Simulations'))

    # Format and display
    yaxis = dict(title=ylabel)
    if scale == 'logarithmic':
        yaxis['type'] = 'log'
    layout = go.Layout(xaxis=dict(title=xlabel),
                       yaxis=yaxis,
                       title=title)
    if jupyter:
        init_notebook_mode(connected=True)
    iplot(go.Figure(data=traces, layout=layout))

    # Return some things in case the user wants to save them for later
    if include_results:
        return cat_pdocs, cat_x
    else:
        return cat_docs, cat_x, ads_docs, ads_x


def _minimize_over(fingerprints, docs, values):
    '''
    In some cases, we do not want to plot all of our data. We would rather
    plot only data that represent minima within certain blocks or fingerprints.
    For example:  If we are looking at adsorption energies, sometimes we only
    want to find the minimum adsorption energies for a given surface for each
    bulk material we are looking at. This function performs this filtering for us.

    We do this by first defining each `block`, where each block is a unique
    combination of fingerprint values within `fingerprints`. For example:  if
    we want to minimize over mpid and miller index, then we would set
    `fingerprints=['mpid', 'miller']`. Then an example of a `block` would be
    `('mpid', [2, 1, 1])`. And then within each block, we would find the data
    point with the minimum `value`, and add only that data point to the output.

    Inputs:
        fingerprints    A list of strings for the fingerprints that you want
                        to block within
        docs            A list of dictionaries, AKA mongo documents.
                        See `gaspy.utils.get_docs` for more details.
        values          A np.array of floats over which to perform the minimization
    Outputs:
        min_pdocs       The filtered version of `docs`
        min_values      The filtered version of `values`
    '''
    # We're going to parse our data into a dictionary, `block_data`, whose keys
    # will be tuples that represent the unique blocks of fingerprints,
    # e.g., ('mpid-23', '[2, 1, 1]'), and whose values will be lists of datum
    # that fall into that bucket. The "datum" will be 2-tuples whose first
    # element is the index of the datum within `values`, and whose second element
    # is the actual `value` from `values`.
    block_data = {}
    for i, value in enumerate(values):
        # Note that we turn the fingerprints into strings to make sure iterables
        # (like lists for miller indices) don't start doing funny things to our code.
        block = tuple([str(docs[i][fingerprint]) for fingerprint in fingerprints])
        # EAFP to either append a new entry to an existing block, or create a new block
        try:
            block_data[block].append((i, value))
        except KeyError:
            block_data[block] = [(i, value)]

    # Now that our data is divided into blocks, we can start figuring out
    # which datum within each block's data set yields the minimum `value`.
    # We will then add the index of that datum as a key to `indices`, which
    # is a search dictionary of indices we'll use to rebuild/filter our data set.
    indices = {}
    for block, data in block_data.iteritems():
        min_block_index = np.argmin([datum[1] for datum in data])
        indices[data[min_block_index][0]] = None

    # Now filter the inputs to create the outputs
    min_values = []
    min_docs = []
    for i, value in enumerate(values):
        if i in indices:
            min_values.append(value)
            min_docs.append(docs[i])
    min_values = np.array(min_values)

    return min_docs, min_values


def _make_trace(x, predictor, docs, label):
    '''
    This function creates a plotly "trace"

    Inputs:
        x           A numpy.array of floats that you want to plot on the x-axis
        predictor   A function that can accept `x` and predict the appropriate values
                    for the y-axis. For example:  This could be a function
                    that can predict activity from adsorption energies.
        docs        The corresponding "mongo documents" that go with `x`.
                    Refer to `gaspy.utils.get_docs` for more details.
        label       A string indicating how you want this trace to be... labeled.
    Output
        trace   A plotly graph object that you can add to a list of traces
    '''
    # Use the predictor to... predict
    y = predictor(x)

    # Concatenate the parsed mongo documents into strings to include as hovertext
    hover_text = []
    for doc in docs:
        text = ''
        for fingerprint, fp_value in doc.iteritems():
            text += '<br>' + str(fingerprint) + ':  ' + str(fp_value)
        hover_text.append(text)

    # Add the catalog data to the plot
    trace = go.Scatter(x=x, y=y, name=label,    # pylint: disable=no-member
                       mode='markers', text=hover_text)

    return trace


def _pull_literature_volcano(excel_file_path, sheetname, scale):
    '''
    This function pulls data from an Excel file to create a "volcano function",
    which is a function that turns an input x-value into an output y-value
    (e.g., adsorption energy to activity). It also outputs the experimental
    data points that should be in the same Excel file.

    Note that we've hard-coded the location of particular information within
    the Excel file, which means that you need to format the Excel file
    appropriately. Follow the template in the repository.

    Inputs:
        excel_file_path     A string indicating the location of the properly-formatted
                            excel file that contains the incumbent volcano
        sheetname           A string indicating the sheet within the Excel file that
                            the information in stored in
        scale               A string indicating the scale across which the volcano curve
                            varies (e.g., linear, logarithmic, etc.).
    Return:
        volcano A function whose input is a float (or a numpy.array) indicating
                the x-axis location on the volcano and whose output
                is a float (or a numpy.array) indicating the y-axis
                location.
        x       The x-values of the experimental data points
        y       The y-values of the experimental data points
        labels  The string-formatted labels of the experimental data points
    '''
    # pylint: disable=no-member
    # Pull the dataframe out of Excel
    df = pd.read_excel(excel_file_path, sheetname=sheetname)
    # Pull out the coordinates (x, y) and the labels (labels) of the
    # experimental data points
    y = df.iloc[:, 0].get_values()
    x = df.iloc[:, 1].get_values()
    labels = df.index.tolist()
    # Do some fancy footwork to find `zenith`, which is the x-value at
    # the zenith of the volcano curve.
    zi = (df.iloc[:, 2] == 'Zenith')
    zenith = df.iloc[:, 3][zi].get_values()[0]
    # Find the slope and intercepts of the lines for both the LHS and
    # RHS of the volcano. Note that this is hard-coded, so make sure
    # the Excel file was completed as per the template.
    lhs_slope = df.iloc[0, 5]
    lhs_intercept = df.iloc[0, 6]
    rhs_slope = df.iloc[0, 9]
    rhs_intercept = df.iloc[0, 10]

    # All of our volcanos are 2-part functions. This `unpack` function returns
    # the parameters that are appropriate for whichever side of the volcano
    # we're on.
    def unpack(x):
        '''
        Unpack either the left-hand-side or right-hand-side parameters from the `parameters`
        dictionary, depending on whether the x-value is to the left or right of the zenith.
        '''
        m = np.where(x < zenith, lhs_slope, rhs_slope)
        b = np.where(x < zenith, lhs_intercept, rhs_intercept)
        return m, b

    # Create the volcano function assuming it's linear
    if scale == 'linear':
        def volcano(x):
            ''' Linear volcano '''
            m, b = unpack(x)
            return m*x + b

    # Create the volcano function assuming it's logarithmic
    elif scale == 'logarithmic':
        def volcano(x):
            ''' Exponential volcano '''
            m, b = unpack(x)
            return np.exp(m*x + b)

    else:
        raise Exception('You have not yet defined the function for a %s scale' % scale)

    return volcano, x, y, labels
